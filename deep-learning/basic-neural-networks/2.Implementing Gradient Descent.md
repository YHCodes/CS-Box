[TOC]

## 2. Implementing Gradient Descent

### 2.1 Mean Squared Error Function

> is the mean of the squares of the differences between the predictions and the labels.

### 2.2 Gradient Descent

神经网络的输出需要尽可能接近真实值，可以用SSE来进行衡量。

我们的目标是寻找神经网络的最优权值

(1) The Sum of Squared Errors, SSE 误差平方和

​	$E=\frac{1}{2}\sum_{\mu}\sum_{j}\left[y_{j}^{\mu}-\hat{y}_{j}^{\mu}\right]^{2}$

- first, take the sum over all output units $j$
- and another sum over all data points $\mu$
-  The square ensures the error is always **positive** and larger errors are penalized more than smaller errors

(2) 已知神经网络的输出

​	 $\hat{y}^{\mu}_j = f(\sum_i w_{ij}x^{\mu}_{i})$

可以得知，SSE取决于 $w_{ij}$

​	$E = \frac{1}{2} \sum_{\mu} \sum_{j} \left[y^{\mu}_{j} - f(\sum_{i}w_{ij}x^{\mu}_{i})\right]^2$

我们的目标：**寻找 $w_{ij}$ 使得 $E$ 最小**, 

方法：Gradient Descent

### 2.3 Gradient Descent: The Math

已知， $\hat{y} = f(h)$  where  $h = \sum_{i}w_{i}x_{i}$

​	$\frac{\part E}{\part w_{i}} = \frac{\part}{\part w_{i}} \frac{1}{2} (y - \hat{y})^2$

​		$= -(y - \hat{y}) \frac{\part{\hat{y}}}{\part w_{i}} =  -(y - \hat{y}) f^{\prime}(h)x_{i}$ 

​	$\Delta w = \eta (y - \hat{y}) f^{\prime}(h)x_{i}$



**ERROR TERM:**

​	$\delta = (y - \hat{y}) f^{\prime}(h)$

**Weight update:**

​	$w_{i} = w_{i} + \eta \delta x_{i}$	

**IF MULTIPLE OUTPUT UNITS:**

​	$\delta_{j} = (y_{j} - \hat{y_{j}}) f^{\prime}(h_{j})$

​	$\Delta w_{ij} = \eta * \delta_{j}*x_{i}$

### 2.4 Gradient Descent: The Code

```python
import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

def sigmoid_prime(x):
    """
    # Derivative of the sigmoid function
    """
    return sigmoid(x) * (1 - sigmoid(x))

learnrate = 0.5
x = np.array([1, 2. 3, 4])
y = np.array(0.5)

# Initial weights
w = np.array([0.5, -0.5, 0.3, 0.1])

### Calculate one gradient descent step for each weight
### Note: Some steps have been consilated, so there are
###       fewer variable names than in the above sample code

# TODO: Calculate the node's linear combination of inputs and weights
h = np.dot(x, w)

# TODO: Calculate output of neural network
nn_output = sigmoid(h)

# TODO: Calculate error of neural network
error = y - nn_output

# TODO: Calculate the error term
#       Remember, this requires the output gradient, which we haven't
#       specifically added a variable for.
error_term = error * sigmoid_prime(h)

# TODO: Calculate change in weights
del_w = learnrate * error_term * x

print('Neural Network output:')
print(nn_output)
print('Amount of Error:')
print(error)
print('Change in Weights:')
print(del_w)
```

### 2.5 Implementing Gradient Descent

**Implementing the hidden layer**

multiple input units and multiple hidden units, the weights between them will require two indices: $w_{ij}$ where $i$ denotes input units and $j$ are the hidden units.

**Weights matrix**:

- Each **row** in the matrix will correspond to the weights **leading out** of a **single input unit**
- Each **column** will correspond to the weights **leading in** to a **single hidden unint**.

![](/Users/liyuanhui/CS-Box/deep-learning/basic-neural-networks/pics/weights_matrix.png)

**initialize weights:**

```python
n_records, n_inputs = features.shape
n_hidden = 2
weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))
```

**calculate hidden layer:**

$h_{1} = x_{1}w_{11} + x_{2}w_{21} + x_{3}w_{31}$

`hidden_inputs  = np.dot(inputs, weights_input_to_hidden)`

**Making a column vector:**

-  `arr.T` : transpose
  - but for a 1D array, the transpose will return a row vector.
  - alternatively, create arrays with two diemensions, then use `arr.T` to get the column vector.
- `arr[:, None]`: create a column vector

**Quiz:**

implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers. Things to do:

- Calculate the input to the hidden layer.
- Calculate the hidden layer output.
- Calculate the input to the output layer.
- Calculate the output of the network.

```python
import numpy as np

def sigmoid(x):
    """
    Calculate sigmoid
    """
    return 1/(1+np.exp(-x))

# Network size
N_input = 4
N_hidden = 3
N_output = 2

np.random.seed(42)
# Make some fake data
X = np.random.randn(4)

weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))
weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))


# TODO: Make a forward pass through the network
hidden_layer_in = np.dot(X, weights_input_to_hidden)
hidden_layer_out = sigmoid(hidden_layer_in)

print('Hidden-layer Output:')
print(hidden_layer_out)

output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)
output_layer_out = sigmoid(output_layer_in)

print('Output-layer Output:')
print(output_layer_out)
```





### Resources

[Why Momentum Really Works, from distill](https://distill.pub/2017/momentum/)

[可汗学院](https://www.khanacademy.org/math/)



 

​	



